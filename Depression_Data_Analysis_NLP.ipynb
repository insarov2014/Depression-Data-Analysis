{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP30DRGYwdXJQJWsE+aIsSY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/insarov2014/Depression-Data-Analysis/blob/main/Depression_Data_Analysis_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I aim at searching a model which help predict if a message reflects depression hints."
      ],
      "metadata": {
        "id": "FIdi1mkI3pZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref: https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned/data?select=depression_dataset_reddit_cleaned.csv"
      ],
      "metadata": {
        "id": "4rcDCBjZBIqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "0Lw-MKrEkRbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install num2words"
      ],
      "metadata": {
        "id": "oyMjBS6G57kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install autocorrect"
      ],
      "metadata": {
        "id": "b-7b0kKp6AY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "L7q-37zb6GkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from num2words import num2words\n",
        "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from autocorrect import Speller\n",
        "\n",
        "# ML imports:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold\n",
        "import sklearn.linear_model as lm\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# DL imports:\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
        "from torch.utils.data import DataLoader\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "wVQDF8Ol551S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Items:\n",
        "# source_of_dataset: Whether to load the dataset from the package, or from a URL (for the particular dataset in this notebook, both options are available)\n",
        "# json_url: The url for the dataset's .json file\n",
        "# db_name: The db name from HuggingFace that holds the raw data\n",
        "# do_preprocessing: Logical, should preprocessing be performed\n",
        "# do_enhanced_preprocessing: Logical, should the computation-heavy preprocessing be performed\n",
        "# do_feature_eng: Logical\n",
        "# maximize_a_priori: Logocal, should the univariate preliminary feature selection be based on a priori or a postiori stats\n",
        "# num_chosen_features_per_class: Int, for the preliminary feature selection, how many features should be selected per class\n",
        "# test_size: ratio between 0 - 1\n",
        "# feature_eng_details: Either \"TfidfVectorizer\" (for TFIDF feature eng.) or \"CountVectorizer\" (for one hot encoding)\n",
        "# seed: Integer, the random seed used to insure reproducibility of results\n",
        "config_dict = {#'source_of_dataset': \"json\",\n",
        "               #'json_url': \"https://huggingface.co/datasets/medalpaca/medical_meadow_health_advice/raw/main/medical_meadow_health_advice.json\",\n",
        "               #'db_name': \"medalpaca/medical_meadow_health_advice\",\n",
        "               'do_preprocessing': True,\n",
        "               'do_enhanced_preprocessing': False,\n",
        "               'do_feature_eng': True,\n",
        "               'maximize_a_priori': False,\n",
        "               'num_chosen_features_per_class': 200,\n",
        "               'test_size': 0.25,\n",
        "               'feature_eng_details': \"CountVectorizer-binary\",\n",
        "               'ngram_range_min': 1,\n",
        "               'ngram_range_max': 3,\n",
        "               'max_features': 1000,\n",
        "               'seed': 0}\n",
        "\n",
        "# Deep learning training parameters:\n",
        "# See description of input parameters in documentation for transformers.TrainingArguments.\n",
        "lm_training_args = TrainingArguments(\n",
        "    output_dir=\"test_trainer\",\n",
        "    num_train_epochs=4, #2\n",
        "    per_device_train_batch_size=8,\n",
        "    learning_rate=2e-5,\n",
        "    eval_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    report_to=[],  # Disable logging to Weights & Biases or other services\n",
        "    )\n",
        "\n",
        "layers_to_fine_tune = None\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.max_rows', None)\n"
      ],
      "metadata": {
        "id": "96y9pPmjmp09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount my Google drive so you can read them easily\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "RrpPPAizWx7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"infamouscoder/depression-reddit-cleaned\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "dnQgQE-RCHvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "734a74ed"
      },
      "source": [
        "import os\n",
        "\n",
        "# Construct the full path to the CSV file\n",
        "csv_file_path = os.path.join(path, \"depression_dataset_reddit_cleaned.csv\")\n",
        "\n",
        "# Read the CSV file into a pandas DataFrame\n",
        "try:\n",
        "    df = pd.read_csv(csv_file_path)\n",
        "    print(\"Successfully loaded 'depression_dataset_reddit_cleaned.csv':\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{csv_file_path}' was not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while reading the CSV: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "wSzLdx1aYWTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "N0jZnw6FYJm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "1eTdFeU6XbxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.duplicated().values==True].shape"
      ],
      "metadata": {
        "id": "bRrAcmpi8z_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep = df.drop_duplicates().copy()\n",
        "df_prep.shape"
      ],
      "metadata": {
        "id": "HYIeBfOMDNS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask_NaN = df_prep.isnull().any(axis=1)\n",
        "df_prep[mask_NaN].shape"
      ],
      "metadata": {
        "id": "erl50nwBwKYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep['is_depression'].value_counts()"
      ],
      "metadata": {
        "id": "CfuFEN3EaXJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_prep.index[0]"
      ],
      "metadata": {
        "id": "mq3_pILQIUtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_frequent_class = df_prep.index[0]\n",
        "print(\"The most frequent class is:\", most_frequent_class)\n",
        "print(\"And its baseline accuracy is:\", round((df_prep['is_depression'] == most_frequent_class).mean(), 3))"
      ],
      "metadata": {
        "id": "-yPsYDTtfztG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's look for a good model. Even though the data look quite clean, I will still do a bit further cleaning, to get rid of some words such as reflexive pronouns."
      ],
      "metadata": {
        "id": "8wvUFI33G_pg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "GI0Gef1dk36k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def spelling_correction(text):\n",
        "    \"\"\"\n",
        "    Replace misspelled words with the correct spelling.\n",
        "\n",
        "    Input: str\n",
        "    Output: str\n",
        "    \"\"\"\n",
        "    corrector = Speller()\n",
        "    spells = [corrector(word) for word in text.split()]\n",
        "    return \" \".join(spells)\n",
        "\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    \"\"\"\n",
        "    Remove stopwords.\n",
        "\n",
        "    Input: str\n",
        "    Output: str\n",
        "    \"\"\"\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    return \" \".join([word for word in text.split() if word not in stopwords_set])\n",
        "\n",
        "\n",
        "def stemming(text):\n",
        "    \"\"\"\n",
        "    Perform stemming of each word individually.\n",
        "\n",
        "    Input: str\n",
        "    Output: str\n",
        "    \"\"\"\n",
        "    stemmer = PorterStemmer()\n",
        "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def lemmatizing(text):\n",
        "    \"\"\"\n",
        "    Perform lemmatization for each word individually.\n",
        "\n",
        "    Input: str\n",
        "    Output: str\n",
        "    \"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "\n",
        "def preprocessing(input_text):\n",
        "  \"\"\"\n",
        "  This function represents a complete pipeline for text preprocessing.\n",
        "\n",
        "  Input: str\n",
        "  Output: str\n",
        "  \"\"\"\n",
        "  output = input_text\n",
        "  # Lower casing:\n",
        "  output = output.lower()\n",
        "  # Remove punctuations and other special characters:\n",
        "  output = re.sub('[^ A-Za-z0-9]+', '', output)\n",
        "\n",
        "  if config_dict[\"do_enhanced_preprocessing\"]:\n",
        "    # Spelling corrections:\n",
        "    output = spelling_correction(output)\n",
        "\n",
        "  # Remove stop words:\n",
        "  output = remove_stop_words(output)\n",
        "\n",
        "  if config_dict[\"do_enhanced_preprocessing\"]:\n",
        "    # Stemming:\n",
        "    output = stemming(output)\n",
        "    # Lemmatizing:\n",
        "    output = lemmatizing(output)\n",
        "\n",
        "  return output"
      ],
      "metadata": {
        "id": "2OixW59bc3LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_clean = df_prep.copy()\n",
        "if config_dict[\"do_preprocessing\"]:\n",
        "  dataset_clean['clean_text'] = [preprocessing(text) for text in dataset_clean['clean_text']]"
      ],
      "metadata": {
        "id": "c8gndg-1K9iH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_clean[['clean_text', 'is_depression']].head(10).style.set_properties(**{'text-align': 'left'})"
      ],
      "metadata": {
        "id": "BwjUY0OxLlvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#EDA"
      ],
      "metadata": {
        "id": "wS0hCLYcNd6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_clean[\"length of text\"] = dataset_clean['clean_text'].map(len)"
      ],
      "metadata": {
        "id": "kXBKZN9Hnz3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qpPf55PlOt8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12,4), sharey=False, tight_layout=True)\n",
        "\n",
        "bins = 12\n",
        "axs[0].hist(dataset_clean[dataset_clean['is_depression']==0][[\"length of text\"]], bins=bins, alpha=0.5)\n",
        "axs[0].set_title(\"Distribution of string length of class 0\")\n",
        "axs[0].set_ylim(0,1000)\n",
        "axs[0].grid(True)\n",
        "\n",
        "axs[1].hist(dataset_clean[dataset_clean['is_depression']==1][[\"length of text\"]], bins=bins, alpha=0.5)\n",
        "axs[1].set_title(\"Distribution of string length of class 1\")\n",
        "axs[1].set_ylim(0,4000)\n",
        "axs[1].grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "98h9bTK7jxtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems that the messages associated with depression are very lengthy!"
      ],
      "metadata": {
        "id": "cM6SIqML0y6A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I try looking for the words that imply depression and non-depression."
      ],
      "metadata": {
        "id": "Qbgj-k3b1HOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering"
      ],
      "metadata": {
        "id": "94S-bkliQr1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feat_eng_text_df(in_df, text_col, labels_col, config_dict):\n",
        "  if \"CountVectorizer-binary\" == config_dict[\"feature_eng_details\"]:\n",
        "    print(\"Feature Engineering method: Binary (one hot encoding)\")\n",
        "    countvectorizer = CountVectorizer(ngram_range=(config_dict[\"ngram_range_min\"], config_dict[\"ngram_range_max\"]),\n",
        "                                      stop_words='english',\n",
        "                                      max_features=config_dict[\"max_features\"],\n",
        "                                      binary=True)\n",
        "\n",
        "  elif \"CountVectorizer-BOW\" == config_dict[\"feature_eng_details\"]:\n",
        "    print(\"Feature Engineering method: Bag of words\")\n",
        "    countvectorizer = CountVectorizer(ngram_range=(config_dict[\"ngram_range_min\"], config_dict[\"ngram_range_max\"]),\n",
        "                                      stop_words='english',\n",
        "                                      max_features=config_dict[\"max_features\"],\n",
        "                                      binary=False)\n",
        "\n",
        "  out_arr = countvectorizer.fit_transform(in_df[text_col])\n",
        "  count_tokens = countvectorizer.get_feature_names_out()\n",
        "  out_df = pd.DataFrame(data = out_arr.toarray(),columns = count_tokens)\n",
        "  out_df[labels_col] = list(in_df[labels_col])\n",
        "  return out_df\n",
        "\n",
        "\n",
        "if config_dict[\"do_feature_eng\"]:\n",
        "  dataset_feat_eng = feat_eng_text_df(dataset_clean, 'clean_text', 'is_depression', config_dict)\n",
        "else:\n",
        "  # This option isn't being supported, the notebook would fail. This option is\n",
        "  # here to cater for a ML pipeline that uses deep learning language models that consume text, and not engineered features.\n",
        "  dataset_feat_eng = dataset_clean.copy()"
      ],
      "metadata": {
        "id": "GqsYgvi4Nq5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exploring the new numerical features"
      ],
      "metadata": {
        "id": "ChiRVXr3uy1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_feat_eng.head()"
      ],
      "metadata": {
        "id": "urchkIOBN9an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Split to Train/Test"
      ],
      "metadata": {
        "id": "rKTpdOAE9_X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_feat_eng_test = dataset_feat_eng.sample(frac=config_dict[\"test_size\"],random_state=config_dict['seed'])\n",
        "dataset_feat_eng_train = dataset_feat_eng.drop(dataset_feat_eng_test.index)"
      ],
      "metadata": {
        "id": "IlDRAiWhu-Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preliminary statistical analysis and feasibility study"
      ],
      "metadata": {
        "id": "OTSWyJeI-SVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Statistics of features per class:\n",
        "means_by_class = dataset_feat_eng_train.groupby(by=['is_depression']).mean().T.sort_index()\n",
        "means_by_class.head()"
      ],
      "metadata": {
        "id": "vZXhmE2b-Ezw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calc the ratio that reflects statistical dependence:\n",
        "P(class, feature)/(P(class)P(feature))\n",
        "And note that it could be rewritten as:\n",
        "P(class | feature)/P(class)\n",
        "Or equivalently:\n",
        "P(feature | class)/P(feature)\n",
        "\n",
        "*Note:\n",
        "The below calculation is assuming that the numerical features of each text term is binary, only then is the below a probability measure.\n",
        "If another feature method is used, such as BoW or TF/IDF, then the below is not the probability, but a proxy of it."
      ],
      "metadata": {
        "id": "lXTwjOhe-udj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "P_class = sorted([[c, np.mean(dataset_feat_eng['is_depression'] == c)] for c in set(means_by_class.columns)])\n",
        "P_feature = sorted([[f, np.mean(dataset_feat_eng[f] > 0)] for f in dataset_feat_eng.columns if f != 'is_depression'])\n",
        "P_feature_inv = [[f, 1/p] for f, p in P_feature]\n",
        "\n",
        "P_class_arr = np.array(P_class)\n",
        "P_feature_arr = np.array(P_feature)\n",
        "P_feature_inv_arr = np.array(P_feature_inv)\n",
        "# Multiplying a \"column vector\" of feature probablities with a \"line vector\" of\n",
        "# class probilities to get a matrix where each element is a product of probabilities:\n",
        "P_class_prod_P_feature_inv_arr = np.outer(P_feature_inv_arr[:, 1].astype(float), P_class_arr[:, 1].astype(float))\n",
        "\n",
        "P_class_given_feature = means_by_class.copy()\n",
        "for feature_counter in range(len(P_class_given_feature)):\n",
        "  for c in P_class_given_feature.columns:\n",
        "    # Right hand side: P(feature | class) / P(feature)\n",
        "    P_class_given_feature[c][feature_counter] = means_by_class[c][feature_counter] / P_feature_arr[feature_counter, 1].astype(float)"
      ],
      "metadata": {
        "id": "pM4bZtY8-XKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_class_given_feature.sort_values([0], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "ovvwJMfV-4GO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "P_class_given_feature.sort_values([1], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "bajsO77J_DRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two tables reveal that there exist some indicative words for depression associated messages!"
      ],
      "metadata": {
        "id": "X8ndxMmQMAuj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Selection\n",
        "\n",
        "This is a univariate feature selection process.\n",
        "It is based on conditional dependency between a feature being 0/1 and a class being 0/1, thus the mean value of the feature is its probability.\n",
        "Note that the process of feature selection is done on the training set."
      ],
      "metadata": {
        "id": "uTix_JcI_eRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each class, choose the most indicative features.\n",
        "Either maximize the:\n",
        "\n",
        "a-priori distribution P(feature | class), Max Liklihood\n",
        "or\n",
        "a posteriori P(class | feature), MAP"
      ],
      "metadata": {
        "id": "q6jp9noe_iqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_features = []\n",
        "if config_dict[\"maximize_a_priori\"] == True:\n",
        "  classes = means_by_class.columns\n",
        "  for c in classes:\n",
        "    chosen_features += list(means_by_class[c].sort_values(ascending=False).index[:config_dict[\"num_chosen_features_per_class\"] + 1])\n",
        "else:\n",
        "  classes = P_class_given_feature.columns\n",
        "  for c in classes:\n",
        "    chosen_features += list(P_class_given_feature[c].sort_values(ascending=False).index[:config_dict[\"num_chosen_features_per_class\"] + 1])\n",
        "\n",
        "\n",
        "chosen_features = list(set(chosen_features))"
      ],
      "metadata": {
        "id": "FhDSyGpS_MG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chosen_features)"
      ],
      "metadata": {
        "id": "Sd62kZWF_tPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chosen_features"
      ],
      "metadata": {
        "id": "GaTPDKLO_mGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leave only chosen features:\n",
        "Now that we deduced which features are \"important\" based on the train set, we select them for both the train set and the test set.  "
      ],
      "metadata": {
        "id": "Mwq7gnMS_126"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_feat_eng_train_selected = dataset_feat_eng_train.filter(chosen_features + ['is_depression'])\n",
        "dataset_feat_eng_test_selected = dataset_feat_eng_test.filter(chosen_features + ['is_depression'])\n",
        "\n",
        "dataset_feat_eng_train_selected.head()"
      ],
      "metadata": {
        "id": "zsw1qNs4_ozi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the data is shrunk and we obtain a more effective dataset."
      ],
      "metadata": {
        "id": "IRkeA7I4Mpw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_feat_eng_train_selected['is_depression'].value_counts()"
      ],
      "metadata": {
        "id": "VVlGpP6EABU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine Learning"
      ],
      "metadata": {
        "id": "m85t_8ppAxzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We conduct regular ML models first."
      ],
      "metadata": {
        "id": "BHfBpRdCM4yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_feat_eng_train_selected.head()"
      ],
      "metadata": {
        "id": "SNGqK8PpAY2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_features_train = dataset_feat_eng_train_selected.values[:, 0:-1]\n",
        "y_labels_train = dataset_feat_eng_train_selected.values[:, -1]\n",
        "\n",
        "x_features_test = dataset_feat_eng_test_selected.values[:, :-1]\n",
        "y_labels_test = dataset_feat_eng_test_selected.values[:, -1]"
      ],
      "metadata": {
        "id": "pPgvTUGRAJ24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "models = []\n",
        "models.append((\"Random Forest\", RandomForestClassifier(random_state=config_dict['seed'])))\n",
        "models.append((\"LASSO\", lm.LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000, random_state=config_dict['seed'])))\n",
        "models.append((\"KNN\", KNeighborsClassifier()))\n",
        "models.append((\"Decision Tree\", DecisionTreeClassifier(random_state=config_dict['seed'])))\n",
        "models.append((\"SVM\", SVC(gamma='auto', random_state=config_dict['seed'])))\n",
        "\n",
        "results = []\n",
        "names = []\n",
        "best_mean_result = 0\n",
        "best_std_result = 0\n",
        "for name, model in models:\n",
        "  kfold = StratifiedKFold()\n",
        "  cv_results = cross_val_score(model, X=x_features_train, y=y_labels_train, scoring='accuracy', cv=kfold)\n",
        "  results.append(cv_results)\n",
        "  names.append(name)\n",
        "  print(name + \": mean(accuracy)=\" + str(round(np.mean(cv_results), 3)) + \", std(accuracy)=\" + str(round(np.std(cv_results), 3)))\n",
        "  if (best_mean_result < np.mean(cv_results)) or \\\n",
        "    ((best_mean_result == np.mean(cv_results)) and (best_std_result > np.std(cv_results))):\n",
        "    best_mean_result = np.mean(cv_results)\n",
        "    best_std_result = np.std(cv_results)\n",
        "    best_model_name = name\n",
        "    best_model = model\n",
        "print(\"\\nBest model is:\\n\" + best_model_name)"
      ],
      "metadata": {
        "id": "6ILgVaT9A7Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.boxplot(results, labels=names)\n",
        "plt.title(\"Models' results' distribution of accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZpRNfnFhBdx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression with LASSO regulation is the best candidate."
      ],
      "metadata": {
        "id": "UMxo_MBVNKqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = lm.LogisticRegression(solver='liblinear', penalty='l1', max_iter=1000, random_state=config_dict['seed'])\n",
        "params = {\"C\": np.linspace(start=0.001, stop=10, num=20)}\n",
        "grid_search = GridSearchCV(model, params, scoring='accuracy')\n",
        "grid_search.fit(x_features_train, y_labels_train)\n",
        "print(\"The optimal hyperparameter 'C' is:\", grid_search.best_params_[\"C\"])\n"
      ],
      "metadata": {
        "id": "GJviw0gEvA1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = lm.LogisticRegression(C=grid_search.best_params_[\"C\"], max_iter=1000, random_state=config_dict['seed'])\n",
        "model.fit(x_features_train, y_labels_train)"
      ],
      "metadata": {
        "id": "8IDlTAdsC_Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate the ML train results: Use for Design Choices"
      ],
      "metadata": {
        "id": "CgGW-pnWFHMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_estimated = model.predict(x_features_train)\n",
        "accuracy_train = np.mean(y_train_estimated == y_labels_train)\n",
        "baseline_accuracy_train = np.mean(0 == y_labels_train)\n",
        "accuracy_lift_train = 100 * (accuracy_train/baseline_accuracy_train - 1)\n",
        "\n",
        "print(\"Results on the train set for a traditional ML model:\\n-------------------------\")\n",
        "print(\"Baseline (dummy classifier) accuracy:\", round(baseline_accuracy_train, 2))\n",
        "print(\"Current model's accuracy:\", round(accuracy_train, 2))\n",
        "print(\"The accuracy lift is:\", round(accuracy_lift_train), \"%\")"
      ],
      "metadata": {
        "id": "gLo3Jbv0FBxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate the ML test results: Use for presenting performance"
      ],
      "metadata": {
        "id": "bct7mlZ6FToD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_estimated = model.predict(x_features_test)\n",
        "accuracy_test = np.mean(y_test_estimated == y_labels_test)\n",
        "baseline_accuracy_test = np.mean(0 == y_labels_test)\n",
        "accuracy_lift = 100 * (accuracy_test/baseline_accuracy_test - 1)\n",
        "\n",
        "print(\"Results on the test set for a traditional ML model:\\n-------------------------\")\n",
        "print(\"Baseline (dummy classifier) accuracy:\", round(baseline_accuracy_test, 2))\n",
        "print(\"Current model's accuracy:\", round(accuracy_test, 2))\n",
        "print(\"The accuracy lift is:\", round(accuracy_lift), \"%\")\n",
        "\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_labels_test, y_test_estimated))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_labels_test, y_test_estimated))"
      ],
      "metadata": {
        "id": "7W-Yr8HpFMRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results on the test set for a traditional ML model:\n",
        "-------------------------\n",
        "Baseline (dummy classifier) accuracy: 0.5\n",
        "Current model's accuracy: 0.82\n",
        "The accuracy lift is: 64 %\n",
        "\n",
        "Confusion Matrix:\n",
        "[[910  50]\n",
        " [288 664]]\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.76      0.95      0.84       960\n",
        "           1       0.93      0.70      0.80       952\n",
        "\n",
        "    accuracy                           0.82      1912\n",
        "   macro avg       0.84      0.82      0.82      1912\n",
        "weighted avg       0.84      0.82      0.82      1912"
      ],
      "metadata": {
        "id": "1L8H5DTn7ftZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "****\n",
        "# Deep Learning  \n",
        "Applying BERT, a Language Model to Text Classification"
      ],
      "metadata": {
        "id": "X8pAjEWJFxAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formatting our data\n",
        "Adjusting the name of the label column:  \n",
        "The design of the Transformers package requires the dataset's lables column to be named exactly `label`.  \n",
        "In the above part of this notebook, where we did tranditional ML work, we had to pick a column name that **isn't** a natural word. The reason is that when we performed feature engineering, each word/Ngram was allocated its own column named after it. If the word \"label\" just happened to appear in the text, it could have a column called `label` defined for it in the dataframe, which would then **conflict with the labels' column name**.  \n",
        "We no longer have that risk, and we need to comply with Transformers' requirements:  "
      ],
      "metadata": {
        "id": "1sl9cI81F8YV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the tokenizer and the pre-trained Language Model:  \n",
        "\n",
        "*Note about fine tuning with Hugging Face:  \n",
        "As of 2025, Hugging Face's Trainer defaults to log metrics using Weights & Biases. That means it demands an API key for that.  \n",
        "To fine-tune without needing a W&B API key, you can disable this integration by setting an env variable `WANDB_DISABLED=true`.  "
      ],
      "metadata": {
        "id": "BxPX9S-pGOZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export WANDB_DISABLED=true"
      ],
      "metadata": {
        "id": "SBrCW8-3FZNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(dataset_clean['is_depression'].unique())\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "language_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))"
      ],
      "metadata": {
        "id": "oKSXez34GRql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>\n",
        "The previous cell would output a warnin starting with:  \n",
        "`Some weights of the model checkpoint at bert-base-uncased were not used when initializing...`\n",
        "\n",
        ">>\n",
        "It is expected, as the model you imported had its pre-trained classification head (i.e. last neural layer) removed and a new \"fresh\" layer is initialized.  \n",
        "That's what we want, as we seek to train that classification head to suit our dataset. Based on our choice, we may choose to also fine-tune other layers.    "
      ],
      "metadata": {
        "id": "wWiLT-xEIhxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The size of the model's token dictionary: {language_model.config.vocab_size}\")"
      ],
      "metadata": {
        "id": "xIzti3TWGXqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the dataset to three sub-sets:  \n",
        "1. A held-out test set  \n",
        "2. A train set that is split to two:  \n",
        "  2.1 A subset used for training the neural network's parameters  \n",
        "  2.2 A subset used to evaluate the progress of the training  "
      ],
      "metadata": {
        "id": "IfnQHc_GIqsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a training set and a test set\n",
        "test_df = dataset_clean.sample(frac=config_dict[\"test_size\"],random_state=config_dict['seed'])\n",
        "train_df = dataset_clean.drop(test_df.index)\n",
        "\n",
        "# Splitting the train set to \"just train\" and \"training evaluation\" set:\n",
        "train_eval_df = train_df.sample(frac=config_dict[\"test_size\"],random_state=config_dict['seed'])\n",
        "train_train_df = train_df.drop(train_eval_df.index)\n",
        "\n",
        "# Rename the label column to 'label' as required by the Transformers Trainer\n",
        "train_train_df = train_train_df.rename(columns={'is_depression': 'label'})\n",
        "train_eval_df = train_eval_df.rename(columns={'is_depression': 'label'})\n",
        "test_df = test_df.rename(columns={'is_depression': 'label'})\n",
        "\n",
        "# Conver the dataframes to a Dataset format per the Transformers package's requirement:\n",
        "dataset_train_train = Dataset.from_pandas(train_train_df)\n",
        "dataset_train_eval = Dataset.from_pandas(train_eval_df)\n",
        "dataset_test = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "2jda3qv5IlYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order for the LM to process the text, it must be tokenized:"
      ],
      "metadata": {
        "id": "cM_43uxfJlXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['clean_text'], padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_train_tokenized = dataset_train_train.map(tokenize_function, batched=True)\n",
        "train_eval_tokenized = dataset_train_eval.map(tokenize_function, batched=True)\n",
        "test_tokenized = dataset_test.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "4wAeb50LItYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training LM"
      ],
      "metadata": {
        "id": "yXrV-JXjJvlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fine tune our pre-trained Language Model via transformers's Trainer."
      ],
      "metadata": {
        "id": "j1Uji6xmJ0gN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing which neural network layers to fine-tune"
      ],
      "metadata": {
        "id": "N2JOMiMaJ3vK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if layers_to_fine_tune == \"head\":\n",
        "  print(\"Fine-tuning only the classification head!\")\n",
        "  language_model.train()\n",
        "  for name, param in language_model.named_parameters():\n",
        "    # Freeze parameters of all layers except classifier head:\n",
        "    if 'classifier' not in name:\n",
        "        param.requires_grad = False\n",
        "else:\n",
        "  print(\"Fine-tuning the entire neural network!\")"
      ],
      "metadata": {
        "id": "BMDFrsNFJn5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training hyperparameters"
      ],
      "metadata": {
        "id": "LpTT-Fo0KCtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the settings for training our model:"
      ],
      "metadata": {
        "id": "0y_OkN7eKFRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation metric"
      ],
      "metadata": {
        "id": "kL2UjoO3KIKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the evaluation metric for the Language Model fine-tuning:"
      ],
      "metadata": {
        "id": "dSypeY8MKLbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"accuracy\")"
      ],
      "metadata": {
        "id": "ObDhqWc1J98l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting the metric evaluation function for the trainer to utilize:"
      ],
      "metadata": {
        "id": "-sa9fYS_KR_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # As the model returns a pair of logit values for each observation,\n",
        "    # where each of the two logit value reflects the likelihood of each\n",
        "    # class, we want to conver it to a classification:\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "6aiYkpi6KN1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer object"
      ],
      "metadata": {
        "id": "G3TOxDbRKXTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=language_model,\n",
        "    args=lm_training_args,\n",
        "    train_dataset=train_train_tokenized,\n",
        "    eval_dataset=train_eval_tokenized,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "OizOcEj8KUVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine tuning"
      ],
      "metadata": {
        "id": "9sHOdKlfKceM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a726a9ed"
      },
      "source": [
        "%%time\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converting the training log to a dataframe for plotting:"
      ],
      "metadata": {
        "id": "O_NwtJmmN2gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_logs_df = pd.DataFrame(trainer.state.log_history).groupby(\"step\", as_index=False).first()"
      ],
      "metadata": {
        "id": "oURfJGjVKfnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_logs_df"
      ],
      "metadata": {
        "id": "XIoEcEULeVEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_logs_df.plot(x=\"epoch\", y=[\"loss\", \"eval_loss\"])\n",
        "plt.title('Observing the performance as the training progresses')\n",
        "plt.legend(['Train Loss', 'Validation Loss'], loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aX43jY8CN56T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_train_train = trainer.predict(train_train_tokenized)#\n",
        "predictions_train_train = np.argmax(results_train_train[0], axis=-1)\n",
        "\n",
        "accuracy_dl_train = np.mean(predictions_train_train == train_train_df[\"label\"])\n",
        "baseline_accuracy_dl_train = np.mean(most_frequent_class == train_train_df[\"label\"])\n",
        "accuracy_dl_lift_train = 100 * (accuracy_dl_train/baseline_accuracy_dl_train - 1)\n",
        "\n",
        "print(\"Results on the train set for a DL Language Model:\\n----------------------------------------------------\")\n",
        "print(\"Baseline (dummy classifier) accuracy:\", round(baseline_accuracy_dl_train, 2))\n",
        "print(\"Current model's accuracy:\", round(accuracy_dl_train, 2))\n",
        "print(\"The accuracy lift is:\", round(accuracy_dl_lift_train), \"%\")"
      ],
      "metadata": {
        "id": "7rlh3kEON88m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What an improvement!"
      ],
      "metadata": {
        "id": "9oehkxWVOXqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_test = trainer.predict(test_tokenized)\n",
        "predictions_test = np.argmax(results_test[0], axis=-1)\n",
        "\n",
        "accuracy_dl_test = np.mean(predictions_test == test_df[\"label\"])\n",
        "baseline_accuracy_dl_test = np.mean(most_frequent_class == test_df[\"label\"])\n",
        "accuracy_dl_lift = 100 * (accuracy_dl_test/baseline_accuracy_dl_test - 1)\n",
        "\n",
        "print(\"Results on the test set for a DL Language Model:\\n---------------------------------------------------\")\n",
        "print(\"Baseline (dummy classifier) accuracy:\", round(baseline_accuracy_dl_test, 2))\n",
        "print(\"Current model's accuracy:\", round(accuracy_dl_test, 2))\n",
        "print(\"The accuracy lift is:\", round(accuracy_dl_lift), \"%\")\n",
        "\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(test_df[\"label\"], predictions_test))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_df[\"label\"], predictions_test))"
      ],
      "metadata": {
        "id": "W3WUbiS8RjLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results on the test set for a DL Language Model:\n",
        "---------------------------------------------------\n",
        "Baseline (dummy classifier) accuracy: 0.5\n",
        "Current model's accuracy: 0.97\n",
        "The accuracy lift is: 93 %\n",
        "\n",
        "Confusion Matrix:\n",
        "[[945  15]\n",
        " [ 41 911]]\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.96      0.98      0.97       960\n",
        "           1       0.98      0.96      0.97       952\n",
        "\n",
        "    accuracy                           0.97      1912\n",
        "   macro avg       0.97      0.97      0.97      1912\n",
        "weighted avg       0.97      0.97      0.97      1912"
      ],
      "metadata": {
        "id": "ey8-249u7V-Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The language model did a much better job than regular ML models. Now we can confidently make prediction for any messages and tell whether or not the speakers have depression symptom."
      ],
      "metadata": {
        "id": "PrA_iWOoQA9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference: Mastering-NLP-from-Foundations-to-LLMs by Lior Gazit Meysam Ghaffari"
      ],
      "metadata": {
        "id": "n6WqK6-yWpdv"
      }
    }
  ]
}